services:
  redis:
    image: redis:7-alpine
    container_name: redis_queue
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data

  model-server:
    image: tensorflow/serving:latest
    container_name: model_server
    ports:
      - "8501:8501"
    volumes:
      - ./models:/models
    command: "--model_config_file=/models/models.config --rest_api_port=8501"

  worker:
    build: backend
    image: lab-backend
    container_name: prediction_worker
    command: python worker.py
    env_file:
      - .env
    environment:
      - REDIS_URL=redis://redis:6379
      - MODEL_URL=http://model-server:8501/v1/models
      - SKETCH_MODEL_PATH=/models/sketch_model.h5
      - WANDB_API_KEY=${WANDB_API_KEY}
    volumes:
      - ./models:/models:ro
    depends_on:
      - redis
      - model-server
    restart: unless-stopped

  backend:
    build: backend
    image: lab-backend
    container_name: backend_app
    ports:
      - "8080:80"
    env_file:
      - .env
    environment:
      - REDIS_URL=redis://redis:6379
      - MODEL_URL=http://model-server:8501/v1/models
    depends_on:
      - redis
      - model-server

  frontend:
    build: frontend
    image: lab-frontend
    container_name: streamlit_frontend
    ports:
      - "8502:8502"
    environment:
      - API_URL=http://backend
      - STREAMLIT_SERVER_PORT=8502
    depends_on:
      - backend

volumes:
  redis_data:
