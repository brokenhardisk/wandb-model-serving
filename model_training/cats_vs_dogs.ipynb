{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "450ca59d",
   "metadata": {},
   "source": [
    "# Cats vs Dogs Binary Classification - Model Training\n",
    "\n",
    "This notebook demonstrates training a binary image classifier (Cat vs Dog) using TensorFlow/Keras with Weights & Biases (W&B) integration for experiment tracking.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Model Type**: Convolutional Neural Network (CNN)\n",
    "- **Task**: Binary Classification (Cat vs Dog)\n",
    "- **Input Size**: 128x128 RGB images\n",
    "- **Output**: Single sigmoid value (0=Cat, 1=Dog)\n",
    "- **Dataset**: Cats and Dogs Light from Zenodo\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "```\n",
    "Input (128, 128, 3)\n",
    "    ↓\n",
    "Conv2D(32) + MaxPool → (64, 64, 32)\n",
    "    ↓\n",
    "Conv2D(64) + MaxPool → (32, 32, 64)\n",
    "    ↓\n",
    "Conv2D(128) + MaxPool → (16, 16, 128)\n",
    "    ↓\n",
    "Flatten → Dense(128) → Dropout(0.3)\n",
    "    ↓\n",
    "Dense(1, sigmoid) → Binary Output\n",
    "```\n",
    "\n",
    "## Training Features\n",
    "\n",
    "- **Data Augmentation**: Rotation, shifts, shear, zoom, horizontal flip\n",
    "- **Normalization**: Pixel values scaled to [0, 1]\n",
    "- **Optimizer**: Adam (lr=1e-3)\n",
    "- **Loss**: Binary Cross-Entropy\n",
    "- **Callbacks**: \n",
    "  - W&B Metrics Logger\n",
    "  - ReduceLROnPlateau\n",
    "  - EarlyStopping (patience=20)\n",
    "\n",
    "## Export Format\n",
    "\n",
    "The final model is exported as TensorFlow SavedModel format, compatible with TensorFlow Serving for production deployment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "984531aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 0. Install required packages\n",
    "# ----------------------------\n",
    "#!pip install tensorflow wandb pillow --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1122fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 1. Import libraries\n",
    "# ----------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint, WandbCallback\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import wandb\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1977dc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthomas-leonharts\u001b[0m (\u001b[33mthomas-leonharts-fh-technikum-wien\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Besitzer\\Documents\\GitHub\\wandb-model-serving\\cats_vs_dogs\\notebook\\wandb\\run-20251123_175712-hv6a4psy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thomas-leonharts-fh-technikum-wien/cats_vs_dogs/runs/hv6a4psy' target=\"_blank\">run_3</a></strong> to <a href='https://wandb.ai/thomas-leonharts-fh-technikum-wien/cats_vs_dogs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thomas-leonharts-fh-technikum-wien/cats_vs_dogs' target=\"_blank\">https://wandb.ai/thomas-leonharts-fh-technikum-wien/cats_vs_dogs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thomas-leonharts-fh-technikum-wien/cats_vs_dogs/runs/hv6a4psy' target=\"_blank\">https://wandb.ai/thomas-leonharts-fh-technikum-wien/cats_vs_dogs/runs/hv6a4psy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/thomas-leonharts-fh-technikum-wien/cats_vs_dogs/runs/hv6a4psy?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1e65f617680>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 2. Initialize W&B\n",
    "# ----------------------------\n",
    "wandb.login()\n",
    "wandb.init(project=\"cats_vs_dogs\", name=\"run_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d29d20",
   "metadata": {},
   "source": [
    "## Initialize Weights & Biases\n",
    "\n",
    "W&B provides experiment tracking, metric logging, and model versioning. Make sure you have a W&B account and API key ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38bbbec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://zenodo.org/record/5226945/files/cats_dogs_light.zip?download=1\n",
      "\u001b[1m32608921/32608921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
      "ZIP downloaded to: c:\\Users\\Besitzer\\Documents\\GitHub\\wandb-model-serving\\cats_vs_dogs\\notebook\\cats_dogs_light.zip\n",
      "ZIP extracted to: ../data\n",
      "Content moved from ../data\\cats_dogs_light to ../data and original folder removed.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 3. Download ZIP to working directory\n",
    "# ----------------------------\n",
    "work_dir = os.getcwd()\n",
    "url = \"https://zenodo.org/record/5226945/files/cats_dogs_light.zip?download=1\"\n",
    "\n",
    "zip_path = tf.keras.utils.get_file(\n",
    "    fname=\"cats_dogs_light.zip\",\n",
    "    origin=url,\n",
    "    extract=False,\n",
    "    cache_dir=work_dir,\n",
    "    cache_subdir=\"\"\n",
    ")\n",
    "\n",
    "print(f\"ZIP downloaded to: {zip_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Extract ZIP\n",
    "# ----------------------------\n",
    "extract_dir = \"../data\"\n",
    "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "print(f\"ZIP extracted to: {extract_dir}\")\n",
    "\n",
    "# copy content from extracted folder to extract_dir and remove extracted folder\n",
    "extracted_folder = os.path.join(extract_dir, \"cats_dogs_light\")\n",
    "for item in os.listdir(extracted_folder):\n",
    "    s = os.path.join(extracted_folder, item)\n",
    "    d = os.path.join(extract_dir, item)\n",
    "    if os.path.isdir(s):\n",
    "        shutil.copytree(s, d, dirs_exist_ok=True)\n",
    "    else:\n",
    "        shutil.copy2(s, d)\n",
    "shutil.rmtree(extracted_folder)\n",
    "print(f\"Content moved from {extracted_folder} to {extract_dir} and original folder removed.\")\n",
    "\n",
    "\n",
    "# remove the zip file to save space\n",
    "os.remove(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ec4311",
   "metadata": {},
   "source": [
    "## Download and Prepare Dataset\n",
    "\n",
    "The dataset is downloaded from Zenodo (cats_dogs_light.zip) and organized into train/test directories with cat/dog subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5221f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images moved to respective category folders.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 5. Within train and test folders create 'cats' and 'dogs' subfolders and move images accordingly\n",
    "# ----------------------------\n",
    "\n",
    "base_dirs = ['train', 'test']\n",
    "categories = ['cats', 'dogs']\n",
    "\n",
    "for base_dir in base_dirs:\n",
    "    base_path = os.path.join(extract_dir, base_dir)\n",
    "    for category in categories:\n",
    "        category_path = os.path.join(base_path, category)\n",
    "        os.makedirs(category_path, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(base_path):\n",
    "        if filename.startswith('cat') and filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            shutil.move(os.path.join(base_path, filename), os.path.join(base_path, 'cats', filename))\n",
    "        elif filename.startswith('dog') and filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            shutil.move(os.path.join(base_path, filename), os.path.join(base_path, 'dogs', filename))\n",
    "print(\"Images moved to respective category folders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96cab3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 6. Image generators\n",
    "# ----------------------------\n",
    "img_size = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "train_dir = os.path.join(extract_dir, \"train\")\n",
    "test_dir = os.path.join(extract_dir, \"test\")\n",
    "\n",
    "# ----------------------------\n",
    "# Image Data Generators\n",
    "# ----------------------------\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    \"../data/train\",\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "val_gen = val_datagen.flow_from_directory(\n",
    "    \"../data/test\",\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff874ecf",
   "metadata": {},
   "source": [
    "## Create Data Generators\n",
    "\n",
    "Data generators handle:\n",
    "- **Training augmentation**: Rotation, shifts, zoom, flips to improve generalization\n",
    "- **Normalization**: Rescaling pixel values to [0, 1]\n",
    "- **Batching**: Load images in batches of 32\n",
    "- **Binary labels**: 0 for cats, 1 for dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7ef6549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cats': 0, 'dogs': 1}\n",
      "{'cats': 0, 'dogs': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_gen.class_indices)  # should show {'0':0, '1':1} or {'cat':0, 'dog':1}\n",
    "print(val_gen.class_indices)  # should show {'0':0, '1':1} or {'cat':0, 'dog':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "957dcbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Besitzer\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32768</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32768\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m4,194,432\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,287,809</span> (16.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,287,809\u001b[0m (16.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,287,809</span> (16.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,287,809\u001b[0m (16.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 7. CNN Model\n",
    "# ----------------------------\n",
    "def build_cnn(input_shape=(128,128,3)):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Block 1\n",
    "    model.add(layers.Conv2D(32, (3,3), activation='relu', padding=\"same\", input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "    # Block 2\n",
    "    model.add(layers.Conv2D(64, (3,3), activation='relu', padding=\"same\"))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "    # Block 3\n",
    "    model.add(layers.Conv2D(128, (3,3), activation='relu', padding=\"same\"))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "    # Dense head\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))   # small dropout is enough\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_cnn()\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceb536c",
   "metadata": {},
   "source": [
    "## Build CNN Model\n",
    "\n",
    "A simple but effective 3-block CNN architecture:\n",
    "- **3 Convolutional Blocks**: Progressive feature extraction (32→64→128 filters)\n",
    "- **Max Pooling**: Spatial dimension reduction\n",
    "- **Dense Layer**: 128 units with ReLU activation\n",
    "- **Dropout**: 0.3 to prevent overfitting\n",
    "- **Output**: Single sigmoid unit for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f70db79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Besitzer\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 645ms/step - accuracy: 0.5450 - loss: 0.7402 - val_accuracy: 0.5000 - val_loss: 0.6901 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 211ms/step - accuracy: 0.5500 - loss: 0.6863 - val_accuracy: 0.5000 - val_loss: 0.6924 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 210ms/step - accuracy: 0.5590 - loss: 0.6808 - val_accuracy: 0.4975 - val_loss: 0.6913 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 214ms/step - accuracy: 0.5600 - loss: 0.6871 - val_accuracy: 0.5125 - val_loss: 0.6814 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 224ms/step - accuracy: 0.5770 - loss: 0.6706 - val_accuracy: 0.5450 - val_loss: 0.6937 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 271ms/step - accuracy: 0.5770 - loss: 0.6720 - val_accuracy: 0.6400 - val_loss: 0.6593 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 251ms/step - accuracy: 0.6100 - loss: 0.6588 - val_accuracy: 0.6425 - val_loss: 0.6261 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 229ms/step - accuracy: 0.6300 - loss: 0.6384 - val_accuracy: 0.6075 - val_loss: 0.6479 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 224ms/step - accuracy: 0.6390 - loss: 0.6469 - val_accuracy: 0.6050 - val_loss: 0.6423 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 235ms/step - accuracy: 0.6560 - loss: 0.6079 - val_accuracy: 0.6975 - val_loss: 0.6211 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 225ms/step - accuracy: 0.6480 - loss: 0.6091 - val_accuracy: 0.6975 - val_loss: 0.6118 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 223ms/step - accuracy: 0.6810 - loss: 0.5928 - val_accuracy: 0.6725 - val_loss: 0.6142 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 245ms/step - accuracy: 0.6900 - loss: 0.5867 - val_accuracy: 0.7075 - val_loss: 0.5746 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 223ms/step - accuracy: 0.6750 - loss: 0.5938 - val_accuracy: 0.7425 - val_loss: 0.5791 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 229ms/step - accuracy: 0.6960 - loss: 0.5677 - val_accuracy: 0.7250 - val_loss: 0.5679 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 224ms/step - accuracy: 0.7200 - loss: 0.5616 - val_accuracy: 0.7250 - val_loss: 0.5880 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 230ms/step - accuracy: 0.6900 - loss: 0.5687 - val_accuracy: 0.6650 - val_loss: 0.6181 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 227ms/step - accuracy: 0.7290 - loss: 0.5408 - val_accuracy: 0.7450 - val_loss: 0.5649 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 233ms/step - accuracy: 0.6880 - loss: 0.5735 - val_accuracy: 0.7350 - val_loss: 0.5301 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 226ms/step - accuracy: 0.7270 - loss: 0.5531 - val_accuracy: 0.7125 - val_loss: 0.5627 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 226ms/step - accuracy: 0.7060 - loss: 0.5484 - val_accuracy: 0.7600 - val_loss: 0.5157 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 228ms/step - accuracy: 0.7290 - loss: 0.5329 - val_accuracy: 0.7650 - val_loss: 0.5146 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 226ms/step - accuracy: 0.7220 - loss: 0.5175 - val_accuracy: 0.7500 - val_loss: 0.5421 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 245ms/step - accuracy: 0.7410 - loss: 0.5066 - val_accuracy: 0.7325 - val_loss: 0.5320 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 234ms/step - accuracy: 0.7380 - loss: 0.5293 - val_accuracy: 0.7650 - val_loss: 0.5302 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 226ms/step - accuracy: 0.7590 - loss: 0.5053 - val_accuracy: 0.7700 - val_loss: 0.5098 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 233ms/step - accuracy: 0.7450 - loss: 0.5140 - val_accuracy: 0.7900 - val_loss: 0.4779 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 234ms/step - accuracy: 0.7520 - loss: 0.4925 - val_accuracy: 0.6900 - val_loss: 0.6475 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 233ms/step - accuracy: 0.7540 - loss: 0.4916 - val_accuracy: 0.7375 - val_loss: 0.5147 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 233ms/step - accuracy: 0.7780 - loss: 0.4784 - val_accuracy: 0.7725 - val_loss: 0.5102 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 232ms/step - accuracy: 0.7630 - loss: 0.4842 - val_accuracy: 0.7875 - val_loss: 0.5191 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 237ms/step - accuracy: 0.7800 - loss: 0.4688 - val_accuracy: 0.7900 - val_loss: 0.4591 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 227ms/step - accuracy: 0.8090 - loss: 0.4338 - val_accuracy: 0.7800 - val_loss: 0.4945 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 235ms/step - accuracy: 0.7980 - loss: 0.4486 - val_accuracy: 0.7975 - val_loss: 0.4577 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 237ms/step - accuracy: 0.8010 - loss: 0.4234 - val_accuracy: 0.8075 - val_loss: 0.4539 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 236ms/step - accuracy: 0.8030 - loss: 0.4288 - val_accuracy: 0.8000 - val_loss: 0.4183 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 255ms/step - accuracy: 0.8080 - loss: 0.4169 - val_accuracy: 0.7950 - val_loss: 0.4202 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 237ms/step - accuracy: 0.7980 - loss: 0.4383 - val_accuracy: 0.8075 - val_loss: 0.4267 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 276ms/step - accuracy: 0.7910 - loss: 0.4425 - val_accuracy: 0.7575 - val_loss: 0.4954 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 273ms/step - accuracy: 0.8040 - loss: 0.4336 - val_accuracy: 0.7625 - val_loss: 0.4846 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 280ms/step - accuracy: 0.8000 - loss: 0.4175 - val_accuracy: 0.7950 - val_loss: 0.4598 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 286ms/step - accuracy: 0.8360 - loss: 0.3542 - val_accuracy: 0.8000 - val_loss: 0.4463 - learning_rate: 2.0000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 279ms/step - accuracy: 0.8380 - loss: 0.3494 - val_accuracy: 0.8150 - val_loss: 0.4109 - learning_rate: 2.0000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 240ms/step - accuracy: 0.8340 - loss: 0.3486 - val_accuracy: 0.8200 - val_loss: 0.4294 - learning_rate: 2.0000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 280ms/step - accuracy: 0.8420 - loss: 0.3481 - val_accuracy: 0.8225 - val_loss: 0.4521 - learning_rate: 2.0000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 284ms/step - accuracy: 0.8610 - loss: 0.3316 - val_accuracy: 0.8250 - val_loss: 0.3968 - learning_rate: 2.0000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 284ms/step - accuracy: 0.8430 - loss: 0.3356 - val_accuracy: 0.8200 - val_loss: 0.4273 - learning_rate: 2.0000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 262ms/step - accuracy: 0.8460 - loss: 0.3359 - val_accuracy: 0.8275 - val_loss: 0.4131 - learning_rate: 2.0000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 268ms/step - accuracy: 0.8490 - loss: 0.3321 - val_accuracy: 0.8375 - val_loss: 0.4030 - learning_rate: 2.0000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 280ms/step - accuracy: 0.8580 - loss: 0.3245 - val_accuracy: 0.8200 - val_loss: 0.4453 - learning_rate: 2.0000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 225ms/step - accuracy: 0.8650 - loss: 0.3235 - val_accuracy: 0.8325 - val_loss: 0.4129 - learning_rate: 2.0000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 227ms/step - accuracy: 0.8820 - loss: 0.2873 - val_accuracy: 0.8300 - val_loss: 0.4275 - learning_rate: 4.0000e-05\n",
      "Epoch 53/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 221ms/step - accuracy: 0.8450 - loss: 0.3453 - val_accuracy: 0.8425 - val_loss: 0.4066 - learning_rate: 4.0000e-05\n",
      "Epoch 54/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 225ms/step - accuracy: 0.8620 - loss: 0.3115 - val_accuracy: 0.8350 - val_loss: 0.4128 - learning_rate: 4.0000e-05\n",
      "Epoch 55/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 226ms/step - accuracy: 0.8610 - loss: 0.3126 - val_accuracy: 0.8300 - val_loss: 0.4077 - learning_rate: 4.0000e-05\n",
      "Epoch 56/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 234ms/step - accuracy: 0.8630 - loss: 0.2984 - val_accuracy: 0.8250 - val_loss: 0.4206 - learning_rate: 4.0000e-05\n",
      "Epoch 57/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 244ms/step - accuracy: 0.8590 - loss: 0.3186 - val_accuracy: 0.8325 - val_loss: 0.4132 - learning_rate: 8.0000e-06\n",
      "Epoch 58/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 257ms/step - accuracy: 0.8660 - loss: 0.3177 - val_accuracy: 0.8300 - val_loss: 0.4148 - learning_rate: 8.0000e-06\n",
      "Epoch 59/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 243ms/step - accuracy: 0.8810 - loss: 0.2975 - val_accuracy: 0.8325 - val_loss: 0.4143 - learning_rate: 8.0000e-06\n",
      "Epoch 60/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 245ms/step - accuracy: 0.8560 - loss: 0.3051 - val_accuracy: 0.8425 - val_loss: 0.4090 - learning_rate: 8.0000e-06\n",
      "Epoch 61/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 244ms/step - accuracy: 0.8650 - loss: 0.3154 - val_accuracy: 0.8350 - val_loss: 0.4097 - learning_rate: 8.0000e-06\n",
      "Epoch 62/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 251ms/step - accuracy: 0.8550 - loss: 0.3130 - val_accuracy: 0.8325 - val_loss: 0.4085 - learning_rate: 1.6000e-06\n",
      "Epoch 63/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 234ms/step - accuracy: 0.8650 - loss: 0.3066 - val_accuracy: 0.8350 - val_loss: 0.4082 - learning_rate: 1.6000e-06\n",
      "Epoch 64/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 230ms/step - accuracy: 0.8740 - loss: 0.3047 - val_accuracy: 0.8350 - val_loss: 0.4083 - learning_rate: 1.6000e-06\n",
      "Epoch 65/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 225ms/step - accuracy: 0.8740 - loss: 0.2973 - val_accuracy: 0.8350 - val_loss: 0.4092 - learning_rate: 1.6000e-06\n",
      "Epoch 66/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 249ms/step - accuracy: 0.8740 - loss: 0.3072 - val_accuracy: 0.8350 - val_loss: 0.4090 - learning_rate: 1.6000e-06\n",
      "Epoch 67/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 237ms/step - accuracy: 0.8710 - loss: 0.2997 - val_accuracy: 0.8350 - val_loss: 0.4092 - learning_rate: 1.0000e-06\n",
      "Epoch 68/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 236ms/step - accuracy: 0.8650 - loss: 0.2989 - val_accuracy: 0.8350 - val_loss: 0.4092 - learning_rate: 1.0000e-06\n",
      "Epoch 69/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 244ms/step - accuracy: 0.8690 - loss: 0.3062 - val_accuracy: 0.8350 - val_loss: 0.4093 - learning_rate: 1.0000e-06\n",
      "Epoch 70/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 250ms/step - accuracy: 0.8600 - loss: 0.3107 - val_accuracy: 0.8325 - val_loss: 0.4095 - learning_rate: 1.0000e-06\n",
      "Epoch 71/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 240ms/step - accuracy: 0.8650 - loss: 0.3067 - val_accuracy: 0.8325 - val_loss: 0.4097 - learning_rate: 1.0000e-06\n",
      "Epoch 72/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 232ms/step - accuracy: 0.8620 - loss: 0.3114 - val_accuracy: 0.8350 - val_loss: 0.4095 - learning_rate: 1.0000e-06\n",
      "Epoch 73/200\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 233ms/step - accuracy: 0.8820 - loss: 0.2977 - val_accuracy: 0.8375 - val_loss: 0.4092 - learning_rate: 1.0000e-06\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 8. Callbacks and Training\n",
    "# ----------------------------\n",
    "\n",
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# Directory to save model checkpoints within W&B run\n",
    "model_dir = wandb.run.dir\n",
    "\n",
    "callbacks = [\n",
    "    WandbMetricsLogger(),  # logs metrics automatically\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6),\n",
    "    # WandbModelCheckpoint(\n",
    "    #     filepath=os.path.join(model_dir, 'cats_vs_dogs_epoch-{epoch:02d}.keras'),\n",
    "    #     monitor='val_loss',\n",
    "    #     verbose=1,\n",
    "    #     save_best_only=False,  # change to True if you only want best model\n",
    "    #     save_weights_only=False,\n",
    "    #     mode='auto',\n",
    "    #     save_freq='epoch'\n",
    "    # ),\n",
    "    EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "# Training with generators\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=200,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438b4572",
   "metadata": {},
   "source": [
    "## Training with Callbacks\n",
    "\n",
    "Training configuration:\n",
    "- **Epochs**: Up to 200 (early stopping will prevent overfitting)\n",
    "- **W&B Metrics Logger**: Automatically logs loss, accuracy, learning rate\n",
    "- **ReduceLROnPlateau**: Reduces learning rate when validation loss plateaus\n",
    "- **EarlyStopping**: Stops training if validation accuracy doesn't improve for 20 epochs\n",
    "\n",
    "All metrics are tracked in W&B for visualization and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c809ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '../model/1'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2088687413456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2088956427920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2088956426000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2088956428112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2088673218768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2088964540560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2088954406736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2088956424080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2088956415632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2088964539024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "SavedModel exported to: ../model/1\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 9. Save final model\n",
    "# ----------------------------\n",
    "# Now export\n",
    "saved_model_path = \"../model/cats_dogs_model/1\"\n",
    "model.export(saved_model_path)\n",
    "print(f\"SavedModel exported to: {saved_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b838a3",
   "metadata": {},
   "source": [
    "## Export Model for Serving\n",
    "\n",
    "The trained model is exported in TensorFlow SavedModel format, which is compatible with TensorFlow Serving.\n",
    "\n",
    "**Export Path**: `../model/cats_dogs_model/1/`\n",
    "\n",
    "This format includes:\n",
    "- `saved_model.pb`: Model architecture and weights\n",
    "- `variables/`: Model parameters\n",
    "- `assets/`: Additional files (if any)\n",
    "\n",
    "### Deploying to Production\n",
    "\n",
    "To serve this model:\n",
    "\n",
    "1. Copy the exported model to the TensorFlow Serving models directory:\n",
    "   ```bash\n",
    "   cp -r ../model/cats_dogs_model/1 /path/to/models/animals/2/\n",
    "   ```\n",
    "\n",
    "2. Update `models.config` to include the new version\n",
    "\n",
    "3. TensorFlow Serving will automatically detect and serve the model\n",
    "\n",
    "### Model Specifications\n",
    "\n",
    "- **Input**: `(batch_size, 128, 128, 3)` - RGB images normalized to [0, 1]\n",
    "- **Output**: `(batch_size, 1)` - Sigmoid probability (0=Cat, 1=Dog)\n",
    "- **Preprocessing Required**: \n",
    "  - Resize to 128x128\n",
    "  - Convert to RGB\n",
    "  - Normalize to [0, 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
